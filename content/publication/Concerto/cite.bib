@inproceedings{cheng2025concerto
author = {
    Shenggan Cheng and
    Shengjie Lin and
    Lansong Diao and
    Hao Wu and
    Siyu Wang and
    Chang Si and
    Ziming Liu and
    Xuanlei Zhao and
    Jiangsu Du and
    Wei Lin and
    Yang You
  },
title = {Concerto: Automatic Communication Optimization and Scheduling for Large-Scale Deep Learning},
year = {2025},
isbn = {0000000000000000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {},
doi = {},
abstract = {With the exponential growth of deep learning (DL), there arises an escalating need for scalability. Despite significant advancements in communication hardware capabilities, the time consumed by communication processes remains a bottleneck during training. The existing various optimizations are coupled within parallel systems to implement specific computation-communication overlap. This approach poses challenges in terms of performance, programmability, and generality. In this paper, we introduce Concerto, a compiler framework designed to address these challenges by automatically optimizing and scheduling communication. We formulate the scheduling problem as a resource constrained project scheduling problem and use off-the-shelf solver to get the near-optimal scheduling. And use auto-decomposition to create overlap opportunity for critical (synchronous) communication. Our evaluation shows Concerto can match or outperform state-of-the-art parallel frameworks, including Megatron-LM, DeepSpeed, and Alpa, all of which include extensive hand-crafted communication optimization. Unlike previous works, Concerto decouples the parallel approach and communication optimization, then can generalize to a wide variety of parallelisms without manual optimization.},
booktitle = {Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {0-0},
numpages = {0},
location = {Rotterdam, The Netherlands},
series = {ASPLOS '25}
}