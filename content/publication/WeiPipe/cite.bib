@inproceedings{lin2025weipipe,
author = {
    Junfeng Lin and
    Liu Ziming and
    Yang You and
    Jun Wang and
    Weihao Zhang and
    Rong Zhao
  },
title = {WeiPipe: Weight Pipeline Parallelism for Communication-Effective Long-Context Large Model Training},
year = {2025},
isbn = {0000000000000000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {},
doi = {},
abstract = {Training large language models (LLMs) has become increasingly expensive due to the rapid expansion in model size. Pipeline Parallelism is a widely used distributed training technique. However, as LLMs with larger context become prevalent and memory optimization techniques advance, traditional PP methods encounter greater communication challenges due to the increased size activations and gradients of activations. To address this issue, we introduce weight-pipeline parallelism (WeiPipe) that transitions from an activation-passing pipeline to a weight-passing pipeline. WeiPipe reduces communication costs and achieves more balanced utilization by transmitting only weights and their gradients between workers in a pipelined manner. WeiPipe does not rely on collective communication primitives, thus ensuring scalability. We present four variations of WeiPipe parallelism, including WeiPipe-Interleave, which emphasizes communication efficiency, and WeiPipe-Zero-Bubble, discussing the potential for minimal bubble ratios. Our implementation of WeiPipe-Interleave, performed on up to 32 GPUs and tested in large-context LLM training, demonstrates up to a 30.9% improvement in throughput with NVLink connections and an 82% improvement with PCIe and IB connections compared to state-of-the-art pipeline parallelism. Additionally, WeiPipe shows greater strong scalability compared to Fully Sharded Data Parallelism.},
booktitle = {Proceedings of ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming 2025},
pages = {0-0},
numpages = {0},
location = {Las Vegas, NV, USA},
series = {PPoPP '25}
}